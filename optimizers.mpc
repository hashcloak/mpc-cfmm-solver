from Compiler import mpc_math

program.use_edabit(True)

sfix.round_nearest = True
cfix.set_precision(f=8, k=34)
sfix.set_precision(f=8, k=34)

program.use_edabit(True)

class Optimizer:

    def __init__(self, fnct, dim):
        self.function = fnct
        self.dimension = dim

    def optimize(self, learning_rate=None):
        pass


class SGDOptimizer(Optimizer):

    def optimize(self, initial_guess, learning_rate=sfix(0.01), n_iter=10):
        guess = sfix(initial_guess)
        @for_range(n_iter)
        def _(i):
            guess.update(guess - learning_rate * self.function.grad(guess))

        return guess

class L_BFGS_BOptimizer(Optimizer):

    def optimize(self, initial_guess, memory_size=17, threshold=cfix(0.000001)):
        k = regint(0)
        size = self.dimension * memory_size
        S = Matrix(self.dimension, memory_size, sfix)
        Y = Matrix(self.dimension, memory_size, sfix)

        guess_k = initial_guess

        invH = Matrix(self.dimension, self.dimension, sfix)
        p_vec_k = Matrix(self.dimension, 1, sfix)

        @while_do(lambda: self.function.norm(self.function.grad(guess_k)) >= threshold)
        def f():
            @if_e(k == 0)
            def _():
                @for_range(self.dimension)
                def _(i):
                    @for_range(self.dimension)
                    def _(j):
                        @if_e(i == j)
                        def _():
                            invH[i][i] = 1
                        @else_
                        def _():
                            invH[i][j] = 0

                grad_f_k = self.function.grad(guess_k)
                p_vec_k.assign(invH.dot(grad_f_k))

            @else_
            def _():
                pass

            k.update(k + 1)

        return (guess_k, self.function.at(guess_k))


class LinearNonnegative:
    def __init__(self, constants):
        tmp = []
        @for_range(len(constants))
        def _(i):
            tmp.append((constants[i] > 0).if_else(True, False))

        for i in tmp:
            if i is False:
                raise Exception
        self.constants = constants

    def grad(self, v):
        tmp = []
        @for_range(len(self.constants))
        def _(i):
            tmp.append((self.constants[i][0] <= v[i][0]).if_else(0, False))

        for i in tmp:
            if i is False:
                raise Exception

        zero_vec = Matrix(len(v), 1, sfix)
        zero_vec.assign_all(0)
        return zero_vec

    def at(self, x):
        zero_vec = Matrix(len(x), 1, sfix)
        zero_vec.assign_all(0)
        return zero_vec

    def norm(self, x):
        return 0


c = sfix.Array(2)
c.assign(sfix(1))
c.assign(sfix(2))

ln = LinearNonnegative(c)
#sgd_opt = SGDOptimizer(ln, 2)
#print_ln("%s", sgd_opt.optimize((sfix(1.5), sfix(1.5))).reveal())

guess = Matrix(2, 1, sfix)
guess.assign(sfix(1.5))
guess.assign(sfix(1.5))
l_bfgs_b_opt = L_BFGS_BOptimizer(ln, 2)
sol = l_bfgs_b_opt.optimize(guess)
print_ln("%s %s", sol[0].reveal(), sol[1].reveal())