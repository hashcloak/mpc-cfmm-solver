from Compiler import mpc_math, ml

"""
For increased optimizations, enable the following parameters when using a ring-based protocol
program.use_edabit(True)
program.use_split(4)
"""

cfix.set_precision(f=32, k=128)
sfix.set_precision(f=32, k=128)

class Optimizer:

    def __init__(self, fnct, dim):
        self.function = fnct
        self.dimension = dim

    def optimize(self, learning_rate=None):
        pass

    def _identity(self):
        I = Matrix(self.dimension, self.dimension, cfix)
        @for_range_opt(self.dimension)
        def _(i):
            @for_range_opt(self.dimension)
            def _(j):
                I[i][j] = (i == j).if_else(cfix(1), cfix(0))
        
        return I


class SGDOptimizer(Optimizer):

    def optimize(self, initial_guess, learning_rate=sfix(0.01), n_iter=10):
        guess = sfix(initial_guess)
        @for_range_opt(n_iter)
        def _(i):
            guess.update(guess - learning_rate * self.function.grad(guess))

        return guess

class L_BFGS_BOptimizer(Optimizer):

    def update_alpha(self, guess_k, p_vec_k, rho, alpha, armijo_c, c_2, n_iter=10, fn=None, g=None):
        alpha_p_vec_k = Matrix(self.dimension, 1, sfix)
        alpha_p_vec_k.assign(alpha * p_vec_k[:])
        alpha_armijo_c_p_vec_k = Matrix(self.dimension, 1, sfix)
        alpha_armijo_c_p_vec_k.assign(armijo_c * alpha_p_vec_k[:])

        tmp1 = Matrix(self.dimension, 1, sfix)
        @for_range_opt(n_iter)
        def _(i):
            tmp1.assign(fn(guess_k) - (fn(guess_k + alpha_p_vec_k)) + g(guess_k).direct_trans_mul(alpha_armijo_c_p_vec_k))
            @if_e((tmp1[:] >= 0).reveal())
            def _():
                tmp2 = Matrix(self.dimension, 1, sfix)
                c2_p_vec_k = Matrix(self.dimension, 1, sfix)
                c2_p_vec_k.assign(-c_2 * p_vec_k[:])
                tmp2.assign(p_vec_k.direct_trans_mul(g(guess_k + alpha_p_vec_k)) + c2_p_vec_k.direct_trans_mul(g(guess_k)))
                @if_((tmp2[:] >= 0).reveal())
                def _():
                    alpha.write(rho * alpha)
                    alpha_p_vec_k.assign(alpha * p_vec_k[:])
                    alpha_armijo_c_p_vec_k.assign((-armijo_c * alpha_p_vec_k[:]))
            @else_
            def _():
                break_loop()

    def optimize(self, initial_guess, fn=None, g=None, threshold=cfix(0.0001), alpha=MemValue(cfix(0.5)), rho=cfix(0.5), armijo_c=cfix(0.0001), c_2=cfix(0.9), n_iter=10):
        k = regint(0)

        guess_k = Matrix(len(initial_guess), 1, sfix)
        guess_k.assign(initial_guess)
        grad_f_k = Matrix(len(initial_guess), 1, sfix)
        prev_guess_k = Matrix(len(initial_guess), 1, sfix)
        prev_grad_k = Matrix(len(initial_guess), 1, sfix)

        invH = Matrix(self.dimension, self.dimension, sfix)
        p_vec_k = Matrix(self.dimension, 1, sfix)
        I = self._identity()

        @for_range_opt(n_iter)
        def _(i):
            @if_e((self.function.norm(g(guess_k)) > threshold).reveal())
            def _():
                @if_(k == 0)
                def _():
                    invH.assign(self._identity())

                invH.print_reveal_nested()

                grad_f_k.assign(g(guess_k))
                p_vec_k.assign(invH.dot(grad_f_k))

                self.update_alpha(guess_k, p_vec_k, rho, alpha, armijo_c, c_2, n_iter=10, fn=fn, g=g)

                prev_guess_k.assign(guess_k)
                tmp = Matrix(self.dimension, 1, sfix)
                tmp.assign(alpha * p_vec_k[:])
                guess_k.assign(prev_guess_k + tmp)
    
                S_k = Matrix(self.dimension, 1, sfix)
                S_k.assign(guess_k - prev_guess_k)

                prev_grad_k.assign(grad_f_k)
                grad_f_k.assign(g(guess_k))
                Y_k = Matrix(self.dimension, 1, sfix)
                Y_k.assign(grad_f_k - prev_grad_k)

                y_kT_s_k = Matrix(Y_k.sizes[0], S_k.sizes[0], sfix)
                y_kT_s_k.assign(Y_k.direct_trans_mul(S_k))
                rho_k = ml.mr(y_kT_s_k, 10)

                s_k_y_kT = Matrix(S_k.sizes[0], Y_k.sizes[0], sfix)
                s_k_y_kT.assign(S_k.direct_mul_trans(Y_k))

                y_k_s_kT = Matrix(Y_k.sizes[0], S_k.sizes[0], sfix)
                y_k_s_kT.assign(Y_k.direct_mul_trans(S_k))
                s_k_s_kT = Matrix(S_k.sizes[0], S_k.sizes[0], sfix)
                s_k_s_kT.assign(S_k.direct_mul_trans(S_k))

                rho_k_s_k_y_kT = Matrix(rho_k.sizes[0], s_k_y_kT.sizes[0], sfix)
                rho_k_s_k_y_kT.assign(rho_k.direct_mul(s_k_y_kT))

                first_operand = Matrix(rho_k_s_k_y_kT.sizes[0], rho_k_s_k_y_kT.sizes[1], sfix)
                first_operand.assign(I - rho_k_s_k_y_kT)

                res1 = Matrix(invH.sizes[0], invH.sizes[1], sfix)
                res1.assign(first_operand.direct_mul(invH))

                second_operand = Matrix(self.dimension, self.dimension, sfix)
                second_operand.assign(rho_k.direct_mul(y_k_s_kT))

                third_operand = Matrix(self.dimension, self.dimension, sfix)
                third_operand.assign(rho_k.direct_mul(s_k_s_kT))
                

                invH.assign(res1.direct_mul((I - second_operand) + third_operand))

                k.update(k + 1)
            
            @else_
            def _():
                break_loop()

        return (guess_k, self.function.at(guess_k))


class LinearNonnegative:
    def __init__(self, constants):
        # NB: Probably shouldn't reveal this
        runtime_error_if((constants[:] <= 0).reveal(), "all elements must be strictly positive")
    
        self.constants = constants
        #return self.constants

    def conjugate(self, v):
        tmp = Matrix(len(v), 1, sfix)
        tmp.assign(v[:][0] - self.constants[:][0])

        zero_vec = Matrix(len(v), 1, sfix)

        @if_e((tmp[:] >= 0).reveal())
        def _():
            zero_vec.assign_all(0)
        @else_
        def _():
            return zero_vec

        return zero_vec

    def grad(self, v):
        tmp = Matrix(len(v), 1, sfix)
        tmp.assign(v[:][0] - self.constants[:][0])

        zero_vec = Matrix(len(v), 1, sfix)

        @if_e((tmp[:] >= 0).reveal())
        def _():
            zero_vec.assign_all(0)
        @else_
        def _():
            return zero_vec

        return zero_vec

    def at(self, x):
        return self.constants

    def norm(self, x):
        s = sum(self.constants)
        norm = mpc_math.sqrt(sfix(s))
        return norm