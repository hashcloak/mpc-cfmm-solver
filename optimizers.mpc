from Compiler import mpc_math, ml
sfix.round_nearest = True
"""
program.use_edabit(True)
program.use_split(4)
"""

cfix.set_precision(f=16, k=40)
sfix.set_precision(f=16, k=40)
"""
program.use_edabit(True)
"""

class Optimizer:

    def __init__(self, fnct, dim):
        self.function = fnct
        self.dimension = dim

    def optimize(self, learning_rate=None):
        pass

    def _identity(self):
        I = Matrix(self.dimension, self.dimension, cfix)
        @for_range_opt(self.dimension)
        def _(i):
            @for_range_opt(self.dimension)
            def _(j):
                I[i][j] = (i == j).if_else(cfix(1), cfix(0))
        
        return I


class SGDOptimizer(Optimizer):

    def optimize(self, initial_guess, learning_rate=sfix(0.01), n_iter=10):
        guess = sfix(initial_guess)
        @for_range_opt(n_iter)
        def _(i):
            guess.update(guess - learning_rate * self.function.grad(guess))

        return guess

class L_BFGS_BOptimizer(Optimizer):

    def update_alpha(self, guess_k, p_vec_k, rho, alpha, armijo_c, c_2, n_iter=10, fn=None, g=None):
        alpha_p_vec_k = Matrix(self.dimension, 1, sfix)
        alpha_p_vec_k.assign(alpha * p_vec_k[:])
        alpha_armijo_c_p_vec_k = Matrix(self.dimension, 1, sfix)
        alpha_armijo_c_p_vec_k.assign(armijo_c * alpha_p_vec_k[:])

        #@while_do(lambda: (tmp[:] >= 0).reveal())
        """
        i = regint(0)
        @do_while
        def _():
            tmp1 = Matrix(self.dimension, 1, sfix)
            #print_ln("%s", tmp.reveal())
            tmp.assign(self.function.at(guess_k) - (self.function.at(guess_k + alpha_p_vec_k)) + self.function.grad(guess_k).direct_trans_mul(alpha_armijo_c_p_vec_k))
            @if_((tmp[:] >= 0).reveal())
            def _():
                i.update(i+1)

            alpha.write(rho * alpha)
            alpha_p_vec_k.assign(alpha * p_vec_k[:])
            alpha_armijo_c_p_vec_k.assign(armijo_c * alpha_p_vec_k[:])
        """

        tmp1 = Matrix(self.dimension, 1, sfix)
        @for_range_opt(n_iter)
        def _(i):
            tmp1.assign(fn(guess_k) - (fn(guess_k + alpha_p_vec_k)) + g(guess_k).direct_trans_mul(alpha_armijo_c_p_vec_k))
            @if_e((tmp1[:] >= 0).reveal())
            def _():
                tmp2 = Matrix(self.dimension, 1, sfix)
                c2_p_vec_k = Matrix(self.dimension, 1, sfix)
                c2_p_vec_k.assign(-c_2 * p_vec_k[:])
                tmp2.assign(p_vec_k.direct_trans_mul(g(guess_k + alpha_p_vec_k)) + c2_p_vec_k.direct_trans_mul(g(guess_k)))
                @if_((tmp2[:] >= 0).reveal())
                def _():
                    #print_ln("rho * alpha: %s", rho * alpha)
                    alpha.write(rho * alpha)
                    alpha_p_vec_k.assign(alpha * p_vec_k[:])
                    alpha_armijo_c_p_vec_k.assign(-armijo_c * alpha_p_vec_k[:])
            @else_
            def _():
                break_loop()

    def optimize(self, initial_guess, fn=None, g=None, threshold=cfix(0.0001), alpha=MemValue(cfix(0.5)), rho=cfix(0.5), armijo_c=cfix(0.0001), c_2=cfix(0.9), n_iter=10):
        k = regint(0)

        guess_k = Matrix(len(initial_guess), 1, sfix)
        guess_k.assign(initial_guess)
        grad_f_k = Matrix(len(initial_guess), 1, sfix)
        grad_f_k.assign(g(guess_k))
        prev_guess_k = Matrix(len(initial_guess), 1, sfix)
        prev_grad_k = Matrix(len(initial_guess), 1, sfix)

        invH = Matrix(self.dimension, self.dimension, sfix)
        p_vec_k = Matrix(self.dimension, 1, sfix)
        I = self._identity()

        @for_range_opt(n_iter)
        def _(i):
            @if_e((self.function.norm(g(guess_k)) > threshold).reveal())
            def _():
                #print_ln("%s", (self.function.norm(self.function.grad(guess_k)) > threshold).reveal())
                #alpha.write(cfix(1))
                @if_(k == 0)
                def _():
                    invH.assign(self._identity())

                grad_f_k = g(guess_k)
                tmp_grad_f_k = Matrix(len(grad_f_k), 1, sfix)
                tmp_grad_f_k.assign(grad_f_k)
                p_vec_k.assign(invH.dot(tmp_grad_f_k))

                self.update_alpha(guess_k, p_vec_k, rho, alpha, armijo_c, c_2, n_iter=10, fn=fn, g=g)

                prev_guess_k.assign(guess_k)
                tmp = Matrix(self.dimension, 1, sfix)
                tmp.assign(alpha * p_vec_k[:])
                guess_k.assign(prev_guess_k + tmp)
    
                S_k = Matrix(self.dimension, 1, sfix)
                S_k.assign(guess_k - prev_guess_k)
                #S_k = Matrix(len(S[k]), 1, S[k].value_type, address=S[k].address)

                prev_grad_k.assign(grad_f_k)
                grad_f_k.assign(g(guess_k))
                Y_k = Matrix(self.dimension, 1, sfix)
                Y_k.assign(grad_f_k - prev_grad_k)
                #Y_k = Matrix(len(Y[k]), 1, Y[k].value_type, address=Y[k].address)

                y_kT_s_k = Matrix(Y_k.sizes[0], S_k.sizes[0], sfix)
                y_kT_s_k.assign(Y_k.direct_trans_mul(S_k))
                rho_k = ml.mr(y_kT_s_k, 10)

                s_k_y_kT = Matrix(S_k.sizes[0], Y_k.sizes[0], sfix)
                s_k_y_kT.assign(S_k.direct_mul_trans(Y_k))

                y_k_s_kT = Matrix(Y_k.sizes[0], S_k.sizes[0], sfix)
                y_k_s_kT.assign(Y_k.direct_mul_trans(S_k))
                s_k_s_kT = Matrix(S_k.sizes[0], S_k.sizes[0], sfix)
                s_k_s_kT.assign(S_k.direct_mul_trans(S_k))

                rho_k_s_k_y_kT = Matrix(rho_k.sizes[0], s_k_y_kT.sizes[0], sfix)
                rho_k_s_k_y_kT.assign(rho_k.direct_mul(s_k_y_kT))

                first_operand = Matrix(rho_k_s_k_y_kT.sizes[0], rho_k_s_k_y_kT.sizes[1], sfix)
                first_operand.assign(I - rho_k_s_k_y_kT)

                res1 = Matrix(invH.sizes[0], invH.sizes[1], sfix)
                res1.assign(first_operand.direct_mul(invH))

                second_operand = Matrix(self.dimension, self.dimension, sfix)
                second_operand.assign(rho_k.direct_mul(y_k_s_kT))

                third_operand = Matrix(self.dimension, self.dimension, sfix)
                third_operand.assign(rho_k.direct_mul(s_k_s_kT))
                

                invH.assign(res1.direct_mul((I - second_operand) + third_operand))

                #print_ln("%s", self.function.norm(self.function.grad(guess_k)).reveal())
                #guess_k.print_reveal_nested()
                k.update(k + 1)
            
            @else_
            def _():
                break_loop()

        return (guess_k, self.function.at(guess_k))


class LinearNonnegative:
    def __init__(self, constants):
        #tmp = []
        #for i in range(len(constants)):
        #    tmp.append((constants[i] > 0).if_else(True, False))

        #for i in tmp:
        #    if i is False:
        #        raise Exception

        # NB: Probably shouldn't reveal this
        #print_ln("%s", constants[:].reveal())
        runtime_error_if((constants[:] <= 0).reveal(), "all elements must be strictly positive")
    
        self.constants = constants
        #return self.constants

    def conjugate(self, v):
        """
        tmp = []
        for i in range(len(constants)):
            tmp.append((self.constants[i][0] <= v[i][0]).if_else(True, False))

        for i in tmp:
            if i is False:
                raise Exception
        """

        tmp = Matrix(len(v), 1, sfix)
        tmp.assign(v[:][0] - self.constants[:][0])
        #tmp.print_reveal_nested()

        zero_vec = Matrix(len(v), 1, sfix)

        @if_e((tmp[:] >= 0).reveal())
        def _():
            zero_vec.assign_all(0)
        @else_
        def _():
        #    runtime_error("Constants must be at most coefficients in v")
            return zero_vec

        return zero_vec
            
        """
        zero_vec = Matrix(len(v), 1, sfix)
        #zero_vec = sfix.Array(2)
        zero_vec.assign_all(0)
        return zero_vec
        """

    def grad(self, v):
        """
        tmp = []
        @for_range(len(self.constants))
        def _(i):
            tmp.append((self.constants[i][0] <= v[i][0]).if_else(True, False))

        for i in tmp:
            if i is False:
                raise Exception

        zero_vec = Matrix(len(v), 1, sfix)
        zero_vec.assign_all(0)
        return zero_vec
        """

        tmp = Matrix(len(v), 1, sfix)
        tmp.assign(v[:][0] - self.constants[:][0])

        zero_vec = Matrix(len(v), 1, sfix)

        @if_e((tmp[:] >= 0).reveal())
        def _():
            zero_vec.assign_all(0)
        @else_
        def _():
        #    runtime_error("Constants must be at most coefficients in v")
            return zero_vec

        return zero_vec

    def at(self, x):
        return self.constants

    def norm(self, x):
        s = sum(self.constants)
        norm = mpc_math.sqrt(s)
        return norm

"""
c = sfix.Array(2)
c.assign(sfix(1))
c.assign(sfix(2))

ln = LinearNonnegative(c)
#sgd_opt = SGDOptimizer(ln, 2)
#print_ln("%s", sgd_opt.optimize((sfix(1.5), sfix(1.5))).reveal())

guess = Matrix(2, 1, sfix)
guess.assign(sfix(1.5))
guess.assign(sfix(1.5))
l_bfgs_b_opt = L_BFGS_BOptimizer(ln, 2)
sol = l_bfgs_b_opt.optimize(guess)
print_ln("%s %s", sol[0].reveal(), sol[1].reveal())
"""