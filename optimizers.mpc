from Compiler import mpc_math, ml

program.use_edabit(True)

sfix.round_nearest = True
cfix.set_precision(f=8, k=34)
sfix.set_precision(f=8, k=34)

program.use_edabit(True)

class Optimizer:

    def __init__(self, fnct, dim):
        self.function = fnct
        self.dimension = dim

    def optimize(self, learning_rate=None):
        pass

    def _identity(self):
        I = Matrix(self.dimension, self.dimension, cfix)
        @for_range_opt(self.dimension)
        def _(i):
            @for_range_opt(self.dimension)
            def _(j):
                I[i][j] = (i == j).if_else(cfix(1), cfix(0))

        return I


class SGDOptimizer(Optimizer):

    def optimize(self, initial_guess, learning_rate=sfix(0.01), n_iter=10):
        guess = sfix(initial_guess)
        @for_range(n_iter)
        def _(i):
            guess.update(guess - learning_rate * self.function.grad(guess))

        return guess

class L_BFGS_BOptimizer(Optimizer):

    def update_alpha(self, point_ap, guess_k, p_vec_k, rho, alpha):
        current_position = self.function.at(point_ap)
        alpha_p_vec_k = Matrix(self.dimension, 1, sfix)
        alpha_p_vec_k.assign(alpha * p_vec_k[:])
        future_position = self.function.at(alpha_p_vec_k) + self.function.grad(guess_k).direct_trans_mul(alpha_p_vec_k)
        tmp = sfix.Array(len(current_position))
        tmp.assign(current_position - future_position)

        @while_do(lambda: (tmp[:] <= 0).reveal())
        def _():
            alpha.write(rho * alpha)
            


    def optimize(self, initial_guess, memory_size=17, threshold=cfix(0.000001), alpha=MemValue(cfix(1)), rho=cfix(0.5), armijo_c=cfix(0.0001), n_iter=10):
        k = regint(0)
        size = self.dimension * memory_size
        #S = Matrix(self.dimension, memory_size, sfix)
        #Y = Matrix(self.dimension, memory_size, sfix)
        #S = []
        #Y = []
        #for i in range(self.dimension):
        #    S.append(Matrix(self.dimension, 1, sfix))
        #    Y.append(Matrix(self.dimension, 1, sfix))

        guess_k = Matrix(len(initial_guess), 1, sfix)
        guess_k.assign(initial_guess)
        grad_f_k = self.function.grad(guess_k)
        prev_guess_k = Matrix(len(initial_guess), 1, sfix)
        prev_grad_k = Matrix(len(initial_guess), 1, sfix)

        invH = Matrix(self.dimension, self.dimension, sfix)
        p_vec_k = Matrix(self.dimension, 1, sfix)

        I = self._identity()
        #alpha = MemValue(alpha)

        # Fine to reveal here since we aren't leaking guess_k
        @while_do(lambda: self.function.norm(self.function.grad(guess_k)).reveal() <= threshold)
        def f():
            @if_(k == 0)
            def _():
                invH = self._identity()

            grad_f_k = self.function.grad(guess_k)
            p_vec_k.assign(invH.dot(grad_f_k))

            # Compute line search using backtracking
            # It's okay to reveal here since we aren't revealing guess_k itself
            point_ap = Matrix(self.dimension, 1, sfix)
            point_ap.assign_vector((alpha * p_vec_k[:]))
            point_ap.iadd(guess_k)

            self.update_alpha(point_ap, guess_k, p_vec_k, rho, alpha)

            #@while_do(lambda: self.function.at(point_ap).reveal() >= (self.function.at(p_vec_k) + self.function.grad(guess_k).direct_trans_mul(p_vec_k)).reveal())
            #def f():
            #    alpha.write(rho * alpha)

            #@for_range(n_iter)
            #def _(i):
            #    if self.function.at(guess_k + p_vec_k).reveal() >= (self.function.at(p_vec_k) + self.function.grad(guess_k).direct_trans_mul(p_vec_k)).reveal():
            #        alpha.write(rho * alpha)

            prev_guess_k.assign(guess_k)
            tmp = Matrix(self.dimension, 1, sfix)
            #alpha_vec = cfix(alpha, size=p_vec_k.sizes[0])

            tmp.assign_vector(alpha * p_vec_k[:])
            guess_k.assign(prev_guess_k + tmp)
            S_k = Matrix(self.dimension, 1, sfix)
            S_k.assign(guess_k - prev_guess_k)
            #S_k = Matrix(len(S[k]), 1, S[k].value_type, address=S[k].address)

            prev_grad_k.assign(grad_f_k)
            grad_f_k.assign(self.function.grad(guess_k))
            Y_k = Matrix(self.dimension, 1, sfix)
            Y_k.assign(grad_f_k - prev_grad_k)
            #Y_k = Matrix(len(Y[k]), 1, Y[k].value_type, address=Y[k].address)

            y_kT_s_k = Matrix(Y_k.sizes[0], S_k.sizes[0], sfix)
            y_kT_s_k.assign(Y_k.direct_trans_mul(S_k))
            rho_k = ml.mr(y_kT_s_k, 10)

            s_k_y_kT = Matrix(S_k.sizes[0], Y_k.sizes[0], sfix)
            s_k_y_kT.assign(S_k.direct_mul_trans(Y_k))

            y_k_s_kT = Matrix(Y_k.sizes[0], S_k.sizes[0], sfix)
            y_k_s_kT.assign(Y_k.direct_mul_trans(S_k))
            s_k_s_kT = Matrix(S_k.sizes[0], S_k.sizes[0], sfix)
            s_k_s_kT.assign(S_k.direct_mul_trans(S_k))

            rho_k_s_k_y_kT = Matrix(rho_k.sizes[0], s_k_y_kT.sizes[0], sfix)
            rho_k_s_k_y_kT.assign(rho_k.direct_mul(s_k_y_kT))

            first_operand = Matrix(rho_k_s_k_y_kT.sizes[0], rho_k_s_k_y_kT.sizes[1], sfix)
            first_operand.assign(I - rho_k_s_k_y_kT)

            res1 = Matrix(invH.sizes[0], invH.sizes[1], sfix)
            res1.assign(first_operand.direct_mul(invH))

            second_operand = Matrix(self.dimension, self.dimension, sfix)
            second_operand.assign(rho_k.direct_mul(y_k_s_kT))

            third_operand = Matrix(self.dimension, self.dimension, sfix)
            third_operand.assign(rho_k.direct_mul(s_k_s_kT))
            

            invH.assign(res1.direct_mul((I - second_operand) + third_operand))


            k.update(k + 1)

        return (guess_k, self.function.at(guess_k))


class LinearNonnegative:
    def __init__(self, constants):
        tmp = []
        for i in range(len(constants)):
            tmp.append((constants[i] > 0).if_else(True, False))

        for i in tmp:
            if i is False:
                raise Exception
        self.constants = constants

    def conjugate(self, v):
        tmp = []
        for i in range(len(constants)):
            tmp.append((self.constants[i][0] <= v[i][0]).if_else(True, False))

        for i in tmp:
            if i is False:
                raise Exception

        zero_vec = Matrix(len(v), 1, sfix)
        zero_vec.assign_all(0)
        return zero_vec

    def grad(self, v):
        tmp = []
        @for_range(len(self.constants))
        def _(i):
            tmp.append((self.constants[i][0] <= v[i][0]).if_else(True, False))

        for i in tmp:
            if i is False:
                raise Exception

        zero_vec = Matrix(len(v), 1, sfix)
        zero_vec.assign_all(0)
        return zero_vec

    def at(self, x):
        return self.constants

    def norm(self, x):
        s = sum(self.constants)
        norm = mpc_math.sqrt(s)
        return norm

"""
c = sfix.Array(2)
c.assign(sfix(1))
c.assign(sfix(2))

ln = LinearNonnegative(c)
#sgd_opt = SGDOptimizer(ln, 2)
#print_ln("%s", sgd_opt.optimize((sfix(1.5), sfix(1.5))).reveal())

guess = Matrix(2, 1, sfix)
guess.assign(sfix(1.5))
guess.assign(sfix(1.5))
l_bfgs_b_opt = L_BFGS_BOptimizer(ln, 2)
sol = l_bfgs_b_opt.optimize(guess)
print_ln("%s %s", sol[0].reveal(), sol[1].reveal())
"""