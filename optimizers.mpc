from Compiler import mpc_math, ml

program.use_edabit(True)

sfix.round_nearest = True
cfix.set_precision(f=8, k=34)
sfix.set_precision(f=8, k=34)

program.use_edabit(True)

class Optimizer:

    def __init__(self, fnct, dim):
        self.function = fnct
        self.dimension = dim

    def optimize(self, learning_rate=None):
        pass

    def _identity(self):
        I = Matrix(self.dimension, self.dimension, cfix)
        @for_range(self.dimension)
        def _(i):
            @for_range(self.dimension)
            def _(j):
                if i == j:
                    I[i][j] = cfix(1)
                else:
                    I[i][j] = cfix(0)

        return I


class SGDOptimizer(Optimizer):

    def optimize(self, initial_guess, learning_rate=sfix(0.01), n_iter=10):
        guess = sfix(initial_guess)
        @for_range(n_iter)
        def _(i):
            guess.update(guess - learning_rate * self.function.grad(guess))

        return guess

class L_BFGS_BOptimizer(Optimizer):

    def optimize(self, initial_guess, memory_size=17, threshold=cfix(0.000001), alpha=cfix(1), rho=cfix(0.5), armijo_c=cfix(0.0001), n_iter=10):
        k = regint(0)
        size = self.dimension * memory_size
        S = Matrix(self.dimension, memory_size, sfix)
        Y = Matrix(self.dimension, memory_size, sfix)

        guess_k = Matrix(len(initial_guess), 1, sfix)
        guess_k.assign(initial_guess)
        grad_f_k = self.function.grad(guess_k)
        prev_guess_k = None
        prev_grad_k = None

        invH = Matrix(self.dimension, self.dimension, sfix)
        p_vec_k = Matrix(self.dimension, 1, sfix)

        I = self._identity()
        alpha = MemValue(sfix(0))

        @while_do(lambda: self.function.norm(self.function.grad(guess_k)) >= threshold)
        def f():
            @if_(k == 0)
            def _():
                @for_range(self.dimension)
                def _(i):
                    @for_range(self.dimension)
                    def _(j):
                        @if_e(i == j)
                        def _():
                            invH[i][i] = 1
                        @else_
                        def _():
                            invH[i][j] = 0

                grad_f_k = self.function.grad(guess_k)
                p_vec_k.assign(invH.dot(grad_f_k))

            # Compute line search using backtracking
            print(type(self.function.grad(guess_k).direct_trans_mul(p_vec_k)))
            @while_do(lambda: self.function.at(guess_k + p_vec_k) >= self.function.at(p_vec_k) + self.function.grad(guess_k).direct_trans_mul(p_vec_k))
            def f():
                alpha.update(rho * alpha)

            prev_guess_k.assign(guess_k)
            tmp = Matrix(self.dimension, 1, sfix)
            tmp.assign_vector(alpha * p_vec_k)
            guess_k.assign(prev_guess_k - tmp)
            s_k = guess_k - prev_guess_k

            prev_grad_k = grad_f_k
            grad_f_k = self.function.grad(guess_k)
            y_k = grad_f_k - prev_grad_k
                
            y_kT_s_k = y_k.direct_trans_mul(s_k)
            y_kT_s_k_inv = ml.mr(y_kT_s_k_inv, 10)
            s_k_y_kT = s_k.direct_mul_trans(y_k)
            y_k_s_kT = y_k.direct_mul_trans(s_k)
            s_k_s_kT = s_k.trans_mul()


            k.update(k + 1)

        return (guess_k, self.function.at(guess_k))


class LinearNonnegative:
    def __init__(self, constants):
        tmp = []
        @for_range(len(constants))
        def _(i):
            tmp.append((constants[i] > 0).if_else(True, False))

        for i in tmp:
            if i is False:
                raise Exception
        self.constants = constants

    def grad(self, v):
        tmp = []
        @for_range(len(self.constants))
        def _(i):
            tmp.append((self.constants[i][0] <= v[i][0]).if_else(0, False))

        for i in tmp:
            if i is False:
                raise Exception

        zero_vec = Matrix(len(v), 1, sfix)
        zero_vec.assign_all(0)
        return zero_vec

    def at(self, x):
        # TODO Implement properly
        return 0

    def norm(self, x):
        # TODO Implement properly
        return 0


c = sfix.Array(2)
c.assign(sfix(1))
c.assign(sfix(2))

ln = LinearNonnegative(c)
#sgd_opt = SGDOptimizer(ln, 2)
#print_ln("%s", sgd_opt.optimize((sfix(1.5), sfix(1.5))).reveal())

guess = Matrix(2, 1, sfix)
guess.assign(sfix(1.5))
guess.assign(sfix(1.5))
l_bfgs_b_opt = L_BFGS_BOptimizer(ln, 2)
sol = l_bfgs_b_opt.optimize(guess)
print_ln("%s %s", sol[0].reveal(), sol[1].reveal())